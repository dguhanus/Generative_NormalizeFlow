{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# In[8]:\n",
        "\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Complete PMU Anomaly Detection with Normalizing Flows\n",
        "No data normalization as requested\n",
        "Handles full cycles data (4000 x 25 x 20 -> 4000 x 500)\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal, MultivariateNormal\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, f1_score, accuracy_score\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\"\"\"\n",
        "Optimized PMU Anomaly Detection with Normalizing Flows\n",
        "Faster training version with reduced complexity\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal, MultivariateNormal\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, f1_score, accuracy_score\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import pickle\n",
        "import time\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== DATA LOADING SECTION ====================\n",
        "print(\"Loading PMU data...\")\n",
        "\n",
        "try:\n",
        "    # Load training data\n",
        "    with open(r\"F:\\Normalizing_Flows\\PowerGrid_Dataset\\PMU_tansient_data\\Train_Test\\Pro2_train_v.pkl\", \"rb\") as f:\n",
        "        train_8000_samples = pickle.load(f)\n",
        "\n",
        "    # Load test data  \n",
        "    with open(r\"F:\\Normalizing_Flows\\PowerGrid_Dataset\\PMU_tansient_data\\Train_Test\\Pro2_test_v.pkl\", \"rb\") as f:\n",
        "        test_2000_samples = pickle.load(f)\n",
        "\n",
        "    print(\"Data loaded successfully!\")\n",
        "    print(f\"Training data type: {type(train_8000_samples)}\")\n",
        "    print(f\"Test data type: {type(test_2000_samples)}\")\n",
        "    \n",
        "    # Debug: Check the structure of the data\n",
        "    if isinstance(train_8000_samples, list):\n",
        "        print(f\"Training data length: {len(train_8000_samples)}\")\n",
        "        if len(train_8000_samples) > 0:\n",
        "            print(f\"First sample type: {type(train_8000_samples[0])}\")\n",
        "            print(f\"First sample shape/length: {np.array(train_8000_samples[0]).shape if hasattr(train_8000_samples[0], '__len__') else 'scalar'}\")\n",
        "    elif isinstance(train_8000_samples, np.ndarray):\n",
        "        print(f\"Training data shape: {train_8000_samples.shape}\")\n",
        "        print(f\"Training data dtype: {train_8000_samples.dtype}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    print(\"Please check if the file paths are correct and files exist.\")\n",
        "    raise e\n",
        "\n",
        "# Convert to numpy arrays and ensure proper data types\n",
        "print(\"Converting data to numpy arrays and checking data types...\")\n",
        "\n",
        "# Handle potential list of lists structure\n",
        "if isinstance(train_8000_samples, list):\n",
        "    print(\"Training data is in list format, converting...\")\n",
        "    np_train_all_cycles = np.array(train_8000_samples, dtype=np.float32)\n",
        "else:\n",
        "    np_train_all_cycles = np.array(train_8000_samples).astype(np.float32)\n",
        "\n",
        "if isinstance(test_2000_samples, list):\n",
        "    print(\"Test data is in list format, converting...\")\n",
        "    np_test_all_cycles = np.array(test_2000_samples, dtype=np.float32)\n",
        "else:\n",
        "    np_test_all_cycles = np.array(test_2000_samples).astype(np.float32)\n",
        "\n",
        "print(f\"Training data shape: {np_train_all_cycles.shape}\")\n",
        "print(f\"Training data dtype: {np_train_all_cycles.dtype}\")\n",
        "print(f\"Test data shape: {np_test_all_cycles.shape}\")\n",
        "print(f\"Test data dtype: {np_test_all_cycles.dtype}\")\n",
        "\n",
        "# Handle NaN/Inf values if present\n",
        "if np.isnan(np_train_all_cycles).sum() > 0:\n",
        "    print(\"Warning: NaN values found in training data. Replacing with zeros.\")\n",
        "    np_train_all_cycles = np.nan_to_num(np_train_all_cycles)\n",
        "\n",
        "if np.isnan(np_test_all_cycles).sum() > 0:\n",
        "    print(\"Warning: NaN values found in test data. Replacing with zeros.\")\n",
        "    np_test_all_cycles = np.nan_to_num(np_test_all_cycles)\n",
        "\n",
        "# Prepare 3D data for all cycles analysis\n",
        "stable_train_data_3d = np_train_all_cycles[:4000].astype(np.float32)      # First 4000 samples (stable)\n",
        "unstable_train_data_3d = np_train_all_cycles[4000:8000].astype(np.float32)  # Last 4000 samples (unstable)\n",
        "stable_test_data_3d = np_test_all_cycles[:1000].astype(np.float32)        # First 1000 test samples (stable) \n",
        "unstable_test_data_3d = np_test_all_cycles[1000:2000].astype(np.float32)   # Last 1000 test samples (unstable)\n",
        "\n",
        "print(f\"Stable train 3D shape: {stable_train_data_3d.shape}\")\n",
        "print(f\"Unstable train 3D shape: {unstable_train_data_3d.shape}\")\n",
        "print(f\"Stable test 3D shape: {stable_test_data_3d.shape}\")\n",
        "print(f\"Unstable test 3D shape: {unstable_test_data_3d.shape}\")\n",
        "\n",
        "# ==================== OPTIMIZED MODEL IMPLEMENTATION ====================\n",
        "\n",
        "class OptimizedCouplingLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Optimized Coupling layer for faster training\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=128, mask_type='checkerboard'):\n",
        "        super(OptimizedCouplingLayer, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Create mask\n",
        "        if mask_type == 'checkerboard':\n",
        "            self.mask = torch.arange(input_dim) % 2\n",
        "        elif mask_type == 'inverse_checkerboard':\n",
        "            self.mask = 1 - torch.arange(input_dim) % 2\n",
        "        else:\n",
        "            indices = torch.randperm(input_dim)\n",
        "            self.mask = torch.zeros(input_dim)\n",
        "            self.mask[indices[:input_dim//2]] = 1\n",
        "        \n",
        "        self.mask = self.mask.float()\n",
        "        \n",
        "        # Simplified networks for faster computation\n",
        "        self.scale_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.Tanh()  # Bound the output for stability\n",
        "        )\n",
        "        \n",
        "        self.translate_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim)\n",
        "        )\n",
        "        \n",
        "        # Initialize weights properly\n",
        "        self._initialize_weights()\n",
        "        \n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize weights for better stability\"\"\"\n",
        "        for module in [self.scale_net, self.translate_net]:\n",
        "            for layer in module:\n",
        "                if isinstance(layer, nn.Linear):\n",
        "                    nn.init.xavier_uniform_(layer.weight, gain=0.1)\n",
        "                    nn.init.zeros_(layer.bias)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass (data to latent)\"\"\"\n",
        "        # Ensure mask is on the same device as input\n",
        "        if self.mask.device != x.device:\n",
        "            self.mask = self.mask.to(x.device)\n",
        "            \n",
        "        masked_x = x * self.mask\n",
        "        \n",
        "        # Compute scale and translation\n",
        "        scale = self.scale_net(masked_x) * 0.5  # Scale down for stability\n",
        "        translate = self.translate_net(masked_x)\n",
        "        \n",
        "        # Apply transformation only to unmasked dimensions\n",
        "        y = x.clone()\n",
        "        inv_mask = 1 - self.mask\n",
        "        y = y * torch.exp(scale * inv_mask) + translate * inv_mask\n",
        "        \n",
        "        # Compute log determinant of Jacobian\n",
        "        log_det_J = torch.sum(scale * inv_mask, dim=1)\n",
        "        \n",
        "        return y, log_det_J\n",
        "    \n",
        "    def inverse(self, y):\n",
        "        \"\"\"Inverse pass (latent to data)\"\"\"\n",
        "        # Ensure mask is on the same device as input\n",
        "        if self.mask.device != y.device:\n",
        "            self.mask = self.mask.to(y.device)\n",
        "            \n",
        "        masked_y = y * self.mask\n",
        "        \n",
        "        # Compute scale and translation\n",
        "        scale = self.scale_net(masked_y) * 0.5\n",
        "        translate = self.translate_net(masked_y)\n",
        "        \n",
        "        # Apply inverse transformation\n",
        "        x = y.clone()\n",
        "        inv_mask = 1 - self.mask\n",
        "        x = (x - translate * inv_mask) * torch.exp(-scale * inv_mask)\n",
        "        \n",
        "        # Compute log determinant of Jacobian (negative of forward)\n",
        "        log_det_J = -torch.sum(scale * inv_mask, dim=1)\n",
        "        \n",
        "        return x, log_det_J\n",
        "\n",
        "class OptimizedNormalizingFlow(nn.Module):\n",
        "    \"\"\"\n",
        "    Optimized Normalizing Flow model for faster training\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_layers=4, hidden_dim=128):\n",
        "        super(OptimizedNormalizingFlow, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        # Create coupling layers with alternating masks\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            mask_type = 'checkerboard' if i % 2 == 0 else 'inverse_checkerboard'\n",
        "            self.layers.append(OptimizedCouplingLayer(input_dim, hidden_dim, mask_type))\n",
        "        \n",
        "        # Use independent normal instead of multivariate for speed\n",
        "        self.register_buffer('base_mean', torch.zeros(input_dim))\n",
        "        self.register_buffer('base_std', torch.ones(input_dim))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass: compute log probability\"\"\"\n",
        "        log_det_J_total = 0\n",
        "        z = x\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            z, log_det_J = layer(z)\n",
        "            log_det_J_total += log_det_J\n",
        "        \n",
        "        # Compute log probability using independent normal (much faster)\n",
        "        base_dist = Normal(self.base_mean, self.base_std)\n",
        "        log_prob_z = base_dist.log_prob(z).sum(dim=1)  # Sum over dimensions\n",
        "        log_prob_x = log_prob_z + log_det_J_total\n",
        "        \n",
        "        return log_prob_x, z\n",
        "    \n",
        "    def sample(self, num_samples):\n",
        "        \"\"\"Generate samples from the learned distribution\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Sample from base distribution\n",
        "            base_dist = Normal(self.base_mean, self.base_std)\n",
        "            z = base_dist.sample((num_samples,))\n",
        "            \n",
        "            # Apply inverse transformations\n",
        "            x = z\n",
        "            for layer in reversed(self.layers):\n",
        "                x, _ = layer.inverse(x)\n",
        "            \n",
        "            return x\n",
        "    \n",
        "    def log_prob(self, x):\n",
        "        \"\"\"Compute log probability of data\"\"\"\n",
        "        log_prob_x, _ = self.forward(x)\n",
        "        return log_prob_x\n",
        "\n",
        "class OptimizedPMUAnomalyDetector:\n",
        "    \"\"\"\n",
        "    Optimized PMU Anomaly Detection for faster training\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=500, num_layers=4, hidden_dim=128, device='cpu'):\n",
        "        self.device = device\n",
        "        self.input_dim = input_dim\n",
        "        \n",
        "        # Use optimized model\n",
        "        self.stable_model = OptimizedNormalizingFlow(input_dim, num_layers, hidden_dim).to(device)\n",
        "        self.loss_history = []\n",
        "        \n",
        "    def prepare_all_cycles_data(self, data_3d):\n",
        "        \"\"\"\n",
        "        Convert 3D data (samples \u00d7 cycles \u00d7 PMUs) to 2D (samples \u00d7 flattened)\n",
        "        Input: (4000, 25, 20) -> Output: (4000, 500)\n",
        "        \"\"\"\n",
        "        # Ensure data is float32\n",
        "        if not isinstance(data_3d, np.ndarray):\n",
        "            data_3d = np.array(data_3d, dtype=np.float32)\n",
        "        else:\n",
        "            data_3d = data_3d.astype(np.float32)\n",
        "            \n",
        "        # Check for valid shape\n",
        "        if len(data_3d.shape) != 3:\n",
        "            raise ValueError(f\"Expected 3D data, got shape: {data_3d.shape}\")\n",
        "            \n",
        "        samples, cycles, pmus = data_3d.shape\n",
        "        print(f\"Converting data from {data_3d.shape} to ({samples}, {cycles * pmus})\")\n",
        "        \n",
        "        # Flatten the cycles and PMUs dimensions\n",
        "        data_2d = data_3d.reshape(samples, cycles * pmus)\n",
        "        \n",
        "        # Handle any remaining NaN/Inf values\n",
        "        data_2d = np.nan_to_num(data_2d, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "        \n",
        "        return data_2d\n",
        "    \n",
        "    def train_model(self, stable_data_3d, epochs=200, lr=2e-4, batch_size=128):\n",
        "        \"\"\"Optimized training with better progress tracking\"\"\"\n",
        "        print(\"Preparing data (converting 4000\u00d725\u00d720 to 4000\u00d7500)...\")\n",
        "        \n",
        "        try:\n",
        "            stable_data_2d = self.prepare_all_cycles_data(stable_data_3d)\n",
        "            stable_tensor = torch.tensor(stable_data_2d, dtype=torch.float32).to(self.device)\n",
        "            print(f\"Training on {stable_tensor.shape[0]} samples with {stable_tensor.shape[1]} features\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error in data preparation: {e}\")\n",
        "            raise e\n",
        "        \n",
        "        # Optimized training setup\n",
        "        optimizer = optim.AdamW(self.stable_model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
        "        \n",
        "        dataset = torch.utils.data.TensorDataset(stable_tensor)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "        \n",
        "        self.stable_model.train()\n",
        "        \n",
        "        print(f\"Starting training for {epochs} epochs with batch size {batch_size}...\")\n",
        "        print(f\"Total batches per epoch: {len(dataloader)}\")\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            num_batches = 0\n",
        "            \n",
        "            for batch_idx, batch in enumerate(dataloader):\n",
        "                batch_data = batch[0]\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                try:\n",
        "                    # Compute negative log likelihood\n",
        "                    log_prob, _ = self.stable_model(batch_data)\n",
        "                    loss = -torch.mean(log_prob)\n",
        "                    \n",
        "                    if torch.isnan(loss) or torch.isinf(loss):\n",
        "                        print(f\"Warning: Invalid loss at epoch {epoch}, batch {batch_idx}\")\n",
        "                        continue\n",
        "                    \n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.stable_model.parameters(), max_norm=1.0)\n",
        "                    optimizer.step()\n",
        "                    \n",
        "                    epoch_loss += loss.item()\n",
        "                    num_batches += 1\n",
        "                    \n",
        "                    # Progress update every 10 batches\n",
        "                    if batch_idx % 10 == 0:\n",
        "                        elapsed = time.time() - start_time\n",
        "                        print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(dataloader)}, \"\n",
        "                              f\"Loss: {loss.item():.4f}, Time: {elapsed:.1f}s\", end='\\r')\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                    continue\n",
        "            \n",
        "            if num_batches > 0:\n",
        "                avg_loss = epoch_loss / num_batches\n",
        "                self.loss_history.append(avg_loss)\n",
        "                scheduler.step()\n",
        "                \n",
        "                # Print epoch summary\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"\\nEpoch {epoch+1}/{epochs} completed - Avg Loss: {avg_loss:.4f}, \"\n",
        "                      f\"Time: {elapsed:.1f}s, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "            \n",
        "        print(f\"\\nTraining completed in {time.time() - start_time:.1f} seconds!\")\n",
        "    \n",
        "    def detect_anomalies_and_localize(self, test_stable_3d, test_unstable_3d, threshold_percentile=5):\n",
        "        \"\"\"\n",
        "        Perform anomaly detection and localization\n",
        "        \"\"\"\n",
        "        print(\"Performing anomaly detection...\")\n",
        "        \n",
        "        # Prepare test data\n",
        "        test_stable_2d = self.prepare_all_cycles_data(test_stable_3d)\n",
        "        test_unstable_2d = self.prepare_all_cycles_data(test_unstable_3d)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        stable_tensor = torch.FloatTensor(test_stable_2d).to(self.device)\n",
        "        unstable_tensor = torch.FloatTensor(test_unstable_2d).to(self.device)\n",
        "        \n",
        "        self.stable_model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Compute log likelihoods\n",
        "            stable_ll = self.stable_model.log_prob(stable_tensor).cpu().numpy()\n",
        "            unstable_ll = self.stable_model.log_prob(unstable_tensor).cpu().numpy()\n",
        "            #stable_ll[stable_ll <0] = np.random.normal(loc=37, scale=1.0)\n",
        "            #unstable_ll[unstable_ll < 0] = np.random.normal(loc=10, scale=1.0)\n",
        "        \n",
        "        # Set threshold based on stable data percentile\n",
        "        threshold = np.percentile(stable_ll, threshold_percentile)\n",
        "        \n",
        "        # Predictions (0 = normal/stable, 1 = anomaly/unstable)\n",
        "        stable_predictions = (stable_ll < threshold).astype(int)\n",
        "        unstable_predictions = (unstable_ll < threshold).astype(int)\n",
        "        \n",
        "        # Combine results\n",
        "        true_labels = np.concatenate([np.zeros(len(test_stable_2d)), np.ones(len(test_unstable_2d))])\n",
        "        predictions = np.concatenate([stable_predictions, unstable_predictions])\n",
        "        scores = np.concatenate([stable_ll, unstable_ll])\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision = precision_score(true_labels, predictions)\n",
        "        recall = recall_score(true_labels, predictions)\n",
        "        f1 = f1_score(true_labels, predictions)\n",
        "        auc_score = roc_auc_score(true_labels, -scores)  # Negative for proper AUC calculation\n",
        "        \n",
        "        # Localization analysis for unstable samples\n",
        "        #localization_results = self.localize_anomalies(test_unstable_3d)\n",
        "        \n",
        "        results = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'auc_score': auc_score,\n",
        "            'threshold': threshold,\n",
        "            'stable_ll': stable_ll,\n",
        "            'unstable_ll': unstable_ll,\n",
        "            'predictions': predictions,\n",
        "            'true_labels': true_labels,\n",
        "            'scores': scores,\n",
        "            #'localization': localization_results\n",
        "        }\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def localize_anomalies(self, unstable_data_3d):\n",
        "        \"\"\"\n",
        "        Localize anomalies by finding PMUs with highest deviations\n",
        "        \"\"\"\n",
        "        print(\"Performing anomaly localization...\")\n",
        "        \n",
        "        localization_results = []\n",
        "        \n",
        "        # Process each unstable sample\n",
        "        for sample_idx in range(unstable_data_3d.shape[0]):\n",
        "            sample_data = unstable_data_3d[sample_idx:sample_idx+1]  # Keep batch dimension\n",
        "            \n",
        "            # Convert to 2D format\n",
        "            sample_2d = self.prepare_all_cycles_data(sample_data)\n",
        "            sample_tensor = torch.FloatTensor(sample_2d).to(self.device)\n",
        "            \n",
        "            self.stable_model.eval()\n",
        "            with torch.no_grad():\n",
        "                # Get the latent representation and intermediate outputs\n",
        "                log_prob, latent = self.stable_model(sample_tensor)\n",
        "                \n",
        "                # Calculate reconstruction error by PMU\n",
        "                pmu_errors = []\n",
        "                \n",
        "                # Analyze each PMU's contribution across all cycles\n",
        "                for pmu_idx in range(20):  # 20 PMUs\n",
        "                    # Extract data for this PMU across all cycles\n",
        "                    pmu_data_across_cycles = unstable_data_3d[sample_idx, :, pmu_idx]  # Shape: (25,)\n",
        "                    \n",
        "                    # Calculate deviation (simple approach - you can make this more sophisticated)\n",
        "                    pmu_error = np.std(pmu_data_across_cycles)\n",
        "                    pmu_errors.append(pmu_error)\n",
        "                \n",
        "                # Find PMU with highest deviation\n",
        "                anomaly_pmu = np.argmax(pmu_errors)\n",
        "                max_deviation = np.max(pmu_errors)\n",
        "                \n",
        "                localization_results.append({\n",
        "                    'sample_idx': sample_idx,\n",
        "                    'anomaly_pmu': anomaly_pmu,\n",
        "                    'max_deviation': max_deviation,\n",
        "                    'pmu_errors': pmu_errors,\n",
        "                    'log_likelihood': log_prob.item()\n",
        "                })\n",
        "        \n",
        "        return localization_results\n",
        "    \n",
        "    def plot_results(self, results):\n",
        "        \"\"\"Plot comprehensive results including probability density plots\"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        \n",
        "        # 1. Training Loss\n",
        "        axes[0,0].plot(self.loss_history)\n",
        "        axes[0,0].set_title('Training Loss (Stable Model)')\n",
        "        axes[0,0].set_xlabel('Epoch')\n",
        "        axes[0,0].set_ylabel('Negative Log Likelihood')\n",
        "        axes[0,0].grid(True)\n",
        "        \n",
        "        # 2. Likelihood Distributions (Probability Density Plot)\n",
        "        axes[0,1].hist(results['stable_ll'], bins=50, alpha=0.7, label='Stable', density=True, color='blue')\n",
        "        axes[0,1].hist(results['unstable_ll'], bins=50, alpha=0.7, label='Unstable', density=True, color='red')\n",
        "        axes[0,1].axvline(results['threshold'], color='green', linestyle='--', label='Threshold')\n",
        "        axes[0,1].set_xlabel('Log Likelihood of stable and unstable data')\n",
        "        axes[0,1].set_ylabel('Histogram')\n",
        "        axes[0,1].set_title('Histogram of log likelihood')\n",
        "        axes[0,1].legend()\n",
        "        axes[0,1].grid(True)\n",
        "        \n",
        "        # 3. ROC Curve\n",
        "        fpr, tpr, _ = roc_curve(results['true_labels'], -results['scores'])\n",
        "        axes[0,2].plot(fpr, tpr, label=f'ROC (AUC = {results[\"auc_score\"]:.3f})')\n",
        "        axes[0,2].plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "        axes[0,2].set_xlabel('False Positive Rate')\n",
        "        axes[0,2].set_ylabel('True Positive Rate')\n",
        "        axes[0,2].set_title('ROC Curve')\n",
        "        axes[0,2].legend()\n",
        "        axes[0,2].grid(True)\n",
        "        \n",
        "        # 4. PMU Anomaly Localization\n",
        "        pmu_anomaly_counts = [0] * 20\n",
        "        for loc_result in results['localization']:\n",
        "            pmu_anomaly_counts[loc_result['anomaly_pmu']] += 1\n",
        "        \n",
        "        axes[1,0].bar(range(20), pmu_anomaly_counts)\n",
        "        axes[1,0].set_xlabel('PMU Index')\n",
        "        axes[1,0].set_ylabel('Anomaly Count')\n",
        "        axes[1,0].set_title('PMU Anomaly Localization')\n",
        "        axes[1,0].grid(True)\n",
        "        \n",
        "        # 5. Deviation Distribution\n",
        "        all_deviations = [loc['max_deviation'] for loc in results['localization']]\n",
        "        axes[1,1].hist(all_deviations, bins=30, alpha=0.7, color='orange')\n",
        "        axes[1,1].set_xlabel('Maximum Deviation')\n",
        "        axes[1,1].set_ylabel('Frequency')\n",
        "        axes[1,1].set_title('Distribution of Maximum Deviations')\n",
        "        axes[1,1].grid(True)\n",
        "        \n",
        "        # 6. Performance Metrics Bar Plot\n",
        "        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
        "        values = [results['accuracy'], results['precision'], results['recall'], \n",
        "                 results['f1_score'], results['auc_score']]\n",
        "        \n",
        "        bars = axes[1,2].bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])\n",
        "        axes[1,2].set_ylabel('Score')\n",
        "        axes[1,2].set_title('Performance Metrics')\n",
        "        axes[1,2].set_ylim(0, 1)\n",
        "        axes[1,2].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for i, v in enumerate(values):\n",
        "            axes[1,2].text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print detailed results\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"OPTIMIZED PMU ANOMALY DETECTION RESULTS\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Accuracy:      {results['accuracy']:.4f}\")\n",
        "        print(f\"Precision:     {results['precision']:.4f}\")\n",
        "        print(f\"Recall:        {results['recall']:.4f}\")\n",
        "        print(f\"F1-Score:      {results['f1_score']:.4f}\")\n",
        "        print(f\"AUC Score:     {results['auc_score']:.4f}\")\n",
        "        print(f\"Threshold:     {results['threshold']:.4f}\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Print top anomalous PMUs\n",
        "        pmu_counts = [0] * 20\n",
        "        for loc in results['localization']:\n",
        "            pmu_counts[loc['anomaly_pmu']] += 1\n",
        "        \n",
        "        print(\"\\nTOP ANOMALOUS PMUs:\")\n",
        "        sorted_pmus = sorted(enumerate(pmu_counts), key=lambda x: x[1], reverse=True)\n",
        "        for i, (pmu_idx, count) in enumerate(sorted_pmus[:5]):\n",
        "            print(f\"PMU {pmu_idx}: {count} anomalies\")\n",
        "    \n",
        "    def save_individual_plots(self, results):\n",
        "        fig=plt.figure(figsize=(6,6))\n",
        "        ax=fig.add_subplot(111)\n",
        "        # plt.figure(figsize=(6,4))\n",
        "        # plt.plot(self.loss_history)\n",
        "        # plt.title('Training Loss (Stable Model)')\n",
        "        # plt.xlabel('Epoch')\n",
        "        # plt.ylabel('Negative Log Likelihood')\n",
        "        # plt.grid(True)\n",
        "        # plt.tight_layout()\n",
        "        # plt.savefig(\"1.png\")\n",
        "        # plt.close()\n",
        "\n",
        "        #ax.figure(figsize=(6,4))\n",
        "        ax.hist(results['stable_ll'], bins=50, alpha=0.7, label='Stable', density=True, color='blue')\n",
        "        ax.hist(results['unstable_ll'], bins=50, alpha=0.7, label='Unstable', density=True, color='red')\n",
        "        ax.axvline(results['threshold'], color='green', linestyle='--', label='Threshold')\n",
        "        plt.xlabel('Log Likelihood of stable and unstable data')\n",
        "        plt.ylabel('Histogram')\n",
        "        plt.title('Plotting log likelihood')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        plt.savefig(\"2.png\")\n",
        "        plt.close()\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(results['true_labels'], -results['scores'])\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(fpr, tpr, label=f'ROC (AUC = {results[\"auc_score\"]:.3f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('ROC Curve')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        plt.savefig(\"3.png\")\n",
        "        plt.close()\n",
        "\n",
        "        # pmu_anomaly_counts = [0] * 20\n",
        "        # for loc_result in results['localization']:\n",
        "        #     pmu_anomaly_counts[loc_result['anomaly_pmu']] += 1\n",
        "\n",
        "        # fig=plt.figure(figsize=(6,6))\n",
        "        # ax=fig.add_subplot(111)\n",
        "        \n",
        "        # #ax.figure(figsize=(6,4))\n",
        "        # ax.bar(range(20), pmu_anomaly_counts)\n",
        "        # plt.xlabel('PMU Index')\n",
        "        # plt.ylabel('Anomaly Count')\n",
        "        # plt.title('PMU Anomaly Localization')\n",
        "        # plt.grid(True)\n",
        "        # plt.tight_layout()\n",
        "        # plt.savefig(\"4.png\")\n",
        "        # plt.close()\n",
        "\n",
        "        # all_deviations = [loc['max_deviation'] for loc in results['localization']]\n",
        "        # fig=plt.figure(figsize=(6,6))\n",
        "        # ax=fig.add_subplot(111)\n",
        "        # #ax.figure(figsize=(6,4))\n",
        "        # ax.hist(all_deviations, bins=30, alpha=0.7, color='orange')\n",
        "        # plt.xlabel('Maximum Deviation')\n",
        "        # plt.ylabel('Frequency')\n",
        "        # plt.title('Distribution of Maximum Deviations')\n",
        "        # plt.grid(True)\n",
        "        # plt.tight_layout()\n",
        "        # plt.savefig(\"5.png\")\n",
        "        # plt.close()\n",
        "\n",
        "        # metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
        "        # values = [results['accuracy'], results['precision'], results['recall'], \n",
        "        #       results['f1_score'], results['auc_score']]\n",
        "\n",
        "        # plt.figure(figsize=(6,4))\n",
        "        # bars = plt.bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])\n",
        "        # plt.ylabel('Score')\n",
        "        # plt.title('Performance Metrics')\n",
        "        # plt.ylim(0, 1)\n",
        "        # plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # for i, v in enumerate(values):\n",
        "        #     plt.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "        # plt.tight_layout()\n",
        "        # plt.savefig(\"6.png\")\n",
        "        # plt.close()\n",
        "\n",
        "# ==================== MAIN EXECUTION ====================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run optimized PMU anomaly detection pipeline\"\"\"\n",
        "    \n",
        "    # Set device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    print(f\"Data shapes:\")\n",
        "    print(f\"- Stable train: {stable_train_data_3d.shape}\")\n",
        "    print(f\"- Unstable train: {unstable_train_data_3d.shape}\") \n",
        "    print(f\"- Stable test: {stable_test_data_3d.shape}\")\n",
        "    print(f\"- Unstable test: {unstable_test_data_3d.shape}\")\n",
        "    \n",
        "    # Initialize optimized detector with reduced complexity\n",
        "    # Input dimension = 25 cycles \u00d7 20 PMUs = 500\n",
        "    detector = OptimizedPMUAnomalyDetector(input_dim=500, num_layers=4, hidden_dim=128, device=device)\n",
        "    \n",
        "    # Train model on stable data only with faster settings\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"OPTIMIZED TRAINING PHASE\")\n",
        "    print(\"=\"*50)\n",
        "    detector.train_model(stable_train_data_3d, epochs=200, lr=2e-4, batch_size=128)\n",
        "    \n",
        "    # Perform detection and localization\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TESTING PHASE\")\n",
        "    print(\"=\"*50)\n",
        "    results = detector.detect_anomalies_and_localize(stable_test_data_3d, unstable_test_data_3d)\n",
        "    \n",
        "    # Plot comprehensive results\n",
        "    #detector.plot_results(results)\n",
        "    detector.save_individual_plots(results)\n",
        "    \n",
        "    return detector, results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    detector, results = main()\n",
        "    \n",
        "\n",
        "\n",
        "# In[7]:\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}